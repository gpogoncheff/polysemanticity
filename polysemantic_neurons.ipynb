{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageNet\n",
    "import lpips\n",
    "import einops\n",
    "from lucent.optvis import render, param, transform, objectives\n",
    "from lucent.modelzoo import inceptionv1\n",
    "import scipy.stats\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial steps in automating the identification of polysemantic neuons in image models\n",
    "\n",
    "Galen Pogoncheff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Feature Visualizations and Dataset Example Analysis for Neurons in InceptionV1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataloaders for ImageNet validation dataset and load pre-trained InceptionV1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "img_ch_means = [0.485, 0.456, 0.406]\n",
    "img_ch_stds = [0.229, 0.224, 0.225]\n",
    "\n",
    "validation_data_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=img_ch_means,\n",
    "        std=img_ch_stds\n",
    "    )\n",
    "])\n",
    "validation_data = ImageNet('./data/imagenet', split='val', transform=validation_data_transform)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=False, num_workers=12, pin_memory=True)\n",
    "\n",
    "model = inceptionv1(pretrained=True)\n",
    "_ = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly sample 128 neuron channels from the 832 channel Mixed4e layer on InceptionV1.  For each image in the ImageNet validation dataset, compute intermediate activations of each sample channel at the Mixed4e layer.  Finally, for each neuron channel, get 25 dataset images that maximized average activation across all neurons in the channel (sorted in descending order of activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:30<00:00, 12.79it/s]\n"
     ]
    }
   ],
   "source": [
    "activations = {}\n",
    "def get_activation(name):\n",
    "    # Hook for recording intermediate layer activations\n",
    "    def hook(model, input, output):\n",
    "        if name not in activations:\n",
    "            activations[name] = []\n",
    "        activations[name].append(output[:,sampled_neurons,:,:].detach().clone().cpu().numpy())\n",
    "    return hook\n",
    "\n",
    "n_neurons = 128\n",
    "sampled_neurons = np.random.permutation(np.arange(832))[:n_neurons]\n",
    "activation_hook = model.mixed4e.register_forward_hook(get_activation('4e'))\n",
    "for input, target in tqdm(validation_dataloader):\n",
    "    input = input.to(device)\n",
    "    target = target.to(device)\n",
    "    _ = model(input)\n",
    "\n",
    "activation_hook.remove()\n",
    "\n",
    "activations['4e'] = np.concatenate(activations['4e'])\n",
    "mean_ch_activations = einops.reduce(activations['4e'], 'n c h w -> n c', 'mean')\n",
    "\n",
    "n_dataset_examples = 25\n",
    "top_n_img_inds = np.empty((n_neurons, n_dataset_examples), dtype=int)\n",
    "for i in range(n_neurons):\n",
    "    neuron_activations = mean_ch_activations[:, i]\n",
    "    top_n_img_inds[i] = np.argsort(neuron_activations)[-n_dataset_examples:][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sampled neuron channel, save dataset examples and feature visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "def plot_img_grid(dataset, img_idxs, img_ch_means=None, img_ch_stds=None, n_rows=5, n_cols=5, fname=None):\n",
    "    '''\n",
    "    Plots a grid of images from a given dataset.\n",
    "\n",
    "    Input:\n",
    "        dataset: torch.utils.data.Dataset of (image, label) pairs\n",
    "        img_means: normalization means to unnormalize the images\n",
    "        img_stds: normalization stds to unnormalize the images\n",
    "        n_rows: number of rows in the grid\n",
    "        n_cols: number of columns in the grid\n",
    "        fname: if not None, saves the figure to the given path\n",
    "\n",
    "    Output:\n",
    "        None\n",
    "    '''\n",
    "    img_idxs = img_idxs[:n_rows*n_cols]\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols, n_rows))\n",
    "    for img_idx, ax in zip(img_idxs, axes.ravel()):\n",
    "        img, label = dataset[img_idx]\n",
    "        img = einops.rearrange(img, 'c h w -> h w c')\n",
    "        img = (img * torch.tensor(img_ch_stds).view(1, 1, 3)) + torch.tensor(img_ch_means).view(1, 1, 3)\n",
    "        ax.imshow(img.numpy())\n",
    "        ax.set_title(label)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    if fname is not None:\n",
    "        plt.savefig(fname, transparent=True)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "for i, neuron in tqdm(enumerate(sampled_neurons)):\n",
    "    plot_img_grid(validation_data, top_n_img_inds[i], img_ch_means, img_ch_stds, 5, 5, f'./test/{neuron}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sampled neuron channel, compute and save 5 diverse feature visualizations using Lucent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for neuron in sampled_neurons:\n",
    "    batch_param_f = lambda: param.image(128, batch=5)\n",
    "    obj = objectives.channel(\"mixed4e\", neuron) - 1e2 * objectives.diversity(\"mixed4e\")\n",
    "    output = render.render_vis(model, obj, batch_param_f, show_inline=False, save_image=True, image_name=f'./figures/{neuron}_featurevis.png')\n",
    "    try:\n",
    "        np.save(f'./data/feature_vis/{neuron}.npy', torch.tensor(output).numpy())\n",
    "    except:\n",
    "        print(f'Failed to save data for neuron {neuron}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual labelings of sampled neuron channels post-manual analysis of dataset examples and feature visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually labelings of sampled neuron channels\n",
    "monos_neurons = [296, 28, 176, 254, 547, 780, 392, 337, 523, 636, \\\n",
    "                 399, 695, 477, 80, 705, 280, 657, 513, 662, 430, \\\n",
    "                 743, 689, 30, 68, 457, 471, 327, 453, 44, 817, 171, \\\n",
    "                 691, 286, 213, 504, 297, 420, 727, 425, 626, 21, 139, \\\n",
    "                 746, 522, 236, 273, 794, 749, 451, 713, 692, 113, 789, \\\n",
    "                 363, 548, 14, 552, 791, 445, 715, 157, 386, 556, 771, \\\n",
    "                 19, 805, 336, 728, 700, 539, 305, 204, 54, 595, 690, \\\n",
    "                 535, 777, 65, 117, 6, 604, 533, 172]\n",
    "\n",
    "polys_neurons = [623, 132, 55, 452, 591, 608, 24, 129, 289, 459, 159, 274, \\\n",
    "                 134, 501, 206, 648, 682, 578, 405, 460, 228, 415, 606, 308, \\\n",
    "                 119, 570, 614, 644, 584, 156]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute pairwise perceptual image similarity losses between among dataset examples for each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpogoncheff/anaconda3/envs/ps_neurons/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/gpogoncheff/anaconda3/envs/ps_neurons/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/gpogoncheff/anaconda3/envs/ps_neurons/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:36<00:00,  3.53it/s]\n"
     ]
    }
   ],
   "source": [
    "neuron_dissimilarities = torch.zeros((n_neurons, int((n_dataset_examples*(n_dataset_examples-1))/2)))\n",
    "\n",
    "lpips_loss = lpips.LPIPS(net='alex').to(device)\n",
    "\n",
    "for neuron_i in tqdm(range(len(sampled_neurons))):\n",
    "    img_inds = top_n_img_inds[neuron_i]\n",
    "    imgs = torch.stack([validation_data[ind][0] for ind in img_inds])\n",
    "    perceptual_dissimilarity = torch.zeros((n_dataset_examples, n_dataset_examples))\n",
    "    for i in range(perceptual_dissimilarity.shape[0]):\n",
    "        src_img = einops.repeat(imgs[i], 'c h w -> n c h w', n=n_dataset_examples)\n",
    "        src_img = src_img.to(device)\n",
    "        cmp_imgs = imgs.to(device)\n",
    "        perceptual_dissimilarity[i, :] = lpips_loss(src_img, cmp_imgs).view(-1).detach().clone().cpu()\n",
    "        perceptual_dissimilarity[:, i] = perceptual_dissimilarity[i, :]\n",
    "    mask = torch.triu(torch.ones_like(perceptual_dissimilarity), diagonal=1)\n",
    "    neuron_dissimilarities[neuron_i] = perceptual_dissimilarity[mask==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Kruskal-Wallis test to compare pairwise dataset example LPIPS distributions and plot KDE plots for manually labeled monosemantic and polysemantic neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monos_neuron_idxs = np.array([np.where(sampled_neurons == neuron)[0][0] for neuron in monos_neurons])\n",
    "polys_neuron_idxs = np.array([np.where(sampled_neurons == neuron)[0][0] for neuron in polys_neurons])\n",
    "\n",
    "mean_dissimilarities = torch.mean(neuron_dissimilarities, dim=1).numpy()\n",
    "\n",
    "\n",
    "print('Kruskal-Wallis Test for Comparison of Distribution Medians')\n",
    "print(scipy.stats.kstest(mean_dissimilarities[monos_neuron_idxs], mean_dissimilarities[polys_neuron_idxs]))\n",
    "\n",
    "# Mean pairwise dataset examples LPIPS KDEs\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.kdeplot([mean_dissimilarities[monos_neuron_idxs], mean_dissimilarities[polys_neuron_idxs]], shade=True, common_norm=False, ax=ax)\n",
    "ax.set_xlabel('Mean LPIPS Loss', size=12)\n",
    "ax.set_ylabel('Density', size=12)\n",
    "plt.legend(loc='upper right', labels=['Polysemantic', 'Monosemantic'], fontsize=11)\n",
    "ax.set_title('Perceptual Differences Among Dataset Examples', fontsize=13)\n",
    "plt.savefig('/figures/lpips_means_dataset_examples.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Kruskal-Wallis test to compare pairwise feature visualization LPIPS distributions and plot KDE plots for manually labeled monosemantic and polysemantic neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpogoncheff/anaconda3/envs/ps_neurons/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/gpogoncheff/anaconda3/envs/ps_neurons/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/gpogoncheff/anaconda3/envs/ps_neurons/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:04, 59.11it/s]\n"
     ]
    }
   ],
   "source": [
    "lpips_img_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "n_fv_examples = 5\n",
    "\n",
    "neuron_fv_dissimilarities = torch.zeros((n_neurons, int((n_fv_examples*(n_fv_examples-1))/2)))\n",
    "\n",
    "lpips_loss = lpips.LPIPS(net='alex').to(device)\n",
    "\n",
    "for neuron_i, neuron in tqdm(enumerate(sampled_neurons)):\n",
    "    fvs = np.load(f'./data/feature_vis/{neuron}.npy')[0]\n",
    "    fvs = einops.rearrange(fvs, 'n h w c -> n c h w')\n",
    "    fv_imgs = lpips_img_transform(torch.Tensor(fvs))\n",
    "    perceptual_dissimilarity = torch.zeros((n_fv_examples, n_fv_examples))\n",
    "    for i in range(perceptual_dissimilarity.shape[0]):\n",
    "        src_img = einops.repeat(fv_imgs[i], 'c h w -> n c h w', n=n_fv_examples)\n",
    "        src_img = src_img.to(device)\n",
    "        cmp_imgs = fv_imgs.to(device)\n",
    "        perceptual_dissimilarity[i, :] = lpips_loss(src_img, cmp_imgs).view(-1).detach().clone().cpu()\n",
    "        perceptual_dissimilarity[:, i] = perceptual_dissimilarity[i, :]\n",
    "    mask = torch.triu(torch.ones_like(perceptual_dissimilarity), diagonal=1)\n",
    "    neuron_fv_dissimilarities[neuron_i] = perceptual_dissimilarity[mask==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_neuron_fv_dissimilarities = torch.mean(neuron_fv_dissimilarities, dim=1).numpy()\n",
    "\n",
    "print('Kruskal-Wallis Test for Comparison of Distribution Medians')\n",
    "print(scipy.stats.kstest(mean_neuron_fv_dissimilarities[monos_neuron_idxs], mean_neuron_fv_dissimilarities[polys_neuron_idxs]))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.kdeplot([mean_neuron_fv_dissimilarities[monos_neuron_idxs], mean_neuron_fv_dissimilarities[polys_neuron_idxs]], shade=True, common_norm=False, ax=ax)\n",
    "ax.set_xlabel('Mean LPIPS Loss', size=12)\n",
    "ax.set_ylabel('Density', size=12)\n",
    "plt.legend(loc='upper left', labels=['Polysemantic', 'Monosemantic'], fontsize=11)\n",
    "ax.set_title('Perceptual Differences Among Feature Visualizations', fontsize=13)\n",
    "plt.savefig('./figures/lpips_means_fvs.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Macaque IT Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from macaque_neural.it_data import MajajHong2015Dataset\n",
    "from macaque_neural.train_convex import compute_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataloaders for Majajhong2015 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "img_fpaths_fpath = './data/majajhong2015_data/img_paths.npy'\n",
    "response_fpath = './data/majajhong2015_data/neuron_responses.npy'\n",
    "\n",
    "it_dataset = MajajHong2015Dataset(img_fpaths_fpath, response_fpath, img_transform=it_img_transform)\n",
    "it_train_set, it_val_set = torch.utils.data.random_split(it_dataset, [0.7, 0.3])\n",
    "\n",
    "it_train_dataloader = DataLoader(it_train_set, batch_size=32, shuffle=True)\n",
    "it_val_dataloader = DataLoader(it_val_set, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get pairs of training data of the form (Mixed4e activations, biological neuron firing rates) for each image stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_activations = {}\n",
    "def get_it_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        artificial_activations[name] = output[:,sampled_neurons,:,:].detach().clone().cpu()\n",
    "    return hook\n",
    "\n",
    "it_hook = model.mixed4e.register_forward_hook(get_it_activation('4e'))\n",
    "\n",
    "X, Y, X_report, Y_report = [], [], [], []\n",
    "for img, response in it_train_dataloader:\n",
    "    img = img.to(device)\n",
    "    _ = model(img)\n",
    "    X.append(artificial_activations['4e'])\n",
    "    Y.append(response)\n",
    "\n",
    "for img, response in it_val_dataloader:\n",
    "    img = img.to(device)\n",
    "    _ = model(img)\n",
    "    X_report.append(artificial_activations['4e'])\n",
    "    Y_report.append(response)\n",
    "\n",
    "it_hook.remove()\n",
    "\n",
    "X = torch.concat(X)\n",
    "Y = torch.concat(Y)\n",
    "X_report = torch.concat(X_report)\n",
    "Y_report = torch.concat(Y_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn linear mapping from artificial neuronal activations to biological neuron firing rates (using [code derived from Patrick Mineault's YHIT repository](https://github.com/patrickmineault/your-head-is-there-to-move-you-around))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6943, 0.7952, 0.9137, 0.9415, 0.9494, 0.9571, 0.9621, 0.9615, 0.9571,\n",
      "        0.9486], device='cuda:0')\n",
      "0.55835795\n"
     ]
    }
   ],
   "source": [
    "regression_results, weights = compute_layer(X.view(X.size(0), -1), Y, X_report.view(X_report.size(0), -1), Y_report, pca=-1, method='ridge', device='cuda:0')\n",
    "print(regression_results['corrs_report_mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze weights of monosemantic and polysemantic neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = weights['W']\n",
    "\n",
    "# Each artificial neuron has a weight for each biological neuron, comput maximum weight for each artificial neuron\n",
    "max_weights = np.max(np.abs(W), axis=1)\n",
    "\n",
    "mixed4e_h, mixed4e_w = 14, 14\n",
    "\n",
    "monos_neuron_weights = max_weights[:len(monos_neurons)*mixed4e_h*mixed4e_w]\n",
    "polys_neuron_weights = max_weights[len(monos_neurons)*mixed4e_h*mixed4e_w:]\n",
    "\n",
    "print('Kuskall-Wallis Test for Comparison of Distribution Medians of Neuron Weight Magnitudes')\n",
    "print(scipy.stats.kruskal(monos_neuron_weights, polys_neuron_weights))\n",
    "\n",
    "# Weight histgrams\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.hist(monos_neuron_weights, density=True, alpha=0.5, label='Monosemantic', bins=20)\n",
    "ax.hist(polys_neuron_weights, density=True, alpha=0.5, label='Polysemantic', bins=20)\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_xlabel('Magnitude of Neuron Weight')\n",
    "ax.set_title('Ridge Regression Weights')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig('./figures/weights.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps_neurons",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
